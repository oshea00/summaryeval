# SummaryEval

A comprehensive evaluation framework for AI-generated summaries that assesses accuracy, completeness, and coherence using NLP techniques and LLM-as-judge methodology.

## Overview

SummaryEval provides quantitative metrics to evaluate the quality of AI-generated summaries across three key dimensions:

- **Accuracy**: Uses "LLM-As-Judge" (openai models) to assess factual correctness and alignment with source material
- **Completeness**: Coverage of important topics from the source text using semantic similarity
- **Coherence**: ROUGE-enhanced scoring combining readability, logical flow, discourse analysis, and internal consistency

## Features

- **CNN/DailyMail Dataset Integration**: Evaluate reference summaries from the CNN/DailyMail dataset
- **Command Line Interface**: Easy-to-use CLI with flexible output formats
- **Multiple Output Formats**: Pretty console output or CSV for data analysis
- **LLM-as-Judge Accuracy**: Uses OpenAI models for sophisticated accuracy assessment
- **Configurable Parameters**: Customize sample size, model selection, and output format

## Environment Setup

Set your OpenAI API key:
```bash
export OPENAI_API_KEY="your-api-key-here"
```

## Quick Start

### Python API

```python
from main import SummaryEvaluator

# Initialize with your preferred OpenAI model
evaluator = SummaryEvaluator("gpt-4o")

# Evaluate a summary
results = evaluator.evaluate_summary(
    ai_summary="Your AI-generated summary here",
    source_text="Original source material here",
    debug=True  # Enable for detailed accuracy rationale
)

print(f"Overall Score: {results['overall']:.3f}")
print(f"Accuracy: {results['accuracy']:.3f}")
print(f"Completeness: {results['completeness']:.3f}")
print(f"Coherence: {results['coherence']:.3f}")
```

### Command Line Interface

Evaluate CNN/DailyMail reference summaries:

```bash
# Basic usage (5 samples, gpt-4o, pretty output)
python evalsummary.py

# Custom parameters
python evalsummary.py -n 10 -m gpt-4 -f pretty

# CSV output for data analysis
python evalsummary.py -n 20 -f csv > results.csv

# Reproducible results with seed
python evalsummary.py -n 5 --seed 42

# Show help
python evalsummary.py --help
```

**CLI Options:**
- `-n, --sample-size`: Number of random articles to evaluate (default: 5)
- `-m, --model`: OpenAI model name (default: gpt-4o)
- `-f, --format`: Output format - `pretty` or `csv` (default: pretty)
- `--seed`: Random seed for reproducible results

## Project Structure

```
summaryeval/
‚îú‚îÄ‚îÄ main.py                      # Core SummaryEvaluator class and test suite
‚îú‚îÄ‚îÄ rouge_coherence_scorer.py    # ROUGE-enhanced coherence scoring module
‚îú‚îÄ‚îÄ evalsummary.py              # CNN/DailyMail evaluation CLI script
‚îú‚îÄ‚îÄ pyproject.toml              # Project dependencies and metadata
‚îú‚îÄ‚îÄ README.md                   # This documentation
‚îî‚îÄ‚îÄ results.csv                 # Example output (generated by running CLI)
```

**Key Files:**
- **main.py**: Contains the `SummaryEvaluator` class with all evaluation methods
- **rouge_coherence_scorer.py**: ROUGE-enhanced coherence scoring using official rouge-score library
- **evalsummary.py**: Command-line interface for evaluating CNN/DailyMail dataset
- **pyproject.toml**: Project configuration with all required dependencies

## Scoring Methodology

### Accuracy Score (0.0 - 1.0)

Uses LLM-as-judge methodology with GPT-4/GPT-4o to assess factual correctness and alignment with source material.

**Process:**
1. **LLM Evaluation**: OpenAI model compares summary against source text
2. **Scoring Scale**: Model rates accuracy on 0-3 scale:
   - **3 (Excellent)**: Highly accurate, well-aligned with source
   - **2 (Good)**: Generally accurate with minor issues
   - **1 (Fair)**: Some accuracy problems or contradictions
   - **0 (Poor)**: Significant inaccuracies or contradictions
3. **Normalization**: Score normalized to 0-1 range (`score / 3.0`)
4. **Rationale**: Model provides reasoning for score (available in debug mode)

**Key Features:**
- Sophisticated semantic understanding via large language models
- Contextual evaluation considering nuance and intent
- Detailed rationale explaining scoring decisions
- Robust handling of complex factual relationships

### Completeness Score (0.0 - 1.0)

Measures coverage of important topics from the source material.

**Process:**
1. **Topic Extraction**: Important topics identified using TF-IDF with position and length weighting:
   - **Position weight**: First/last sentences get 1.5x weight, early/late sentences get 1.2x
   - **Length weight**: Longer sentences (up to 20 words) get higher importance
2. **Coverage Analysis**: Summary embedding compared to topic embeddings
3. **Topic Matching**: Topics covered if similarity >= 0.4 with summary
4. **Weighted Scoring**: Combines topic recall and importance weighting
5. **Final Score**: `completeness = (coverage_recall * 0.7) + (importance_weighting * 0.3)`

**Key Parameters:**
- Coverage threshold: 0.4
- Recall weight: 0.7
- Importance weight: 0.3

### Coherence Score (0.0 - 1.0)

Uses ROUGE-enhanced scoring to measure readability, logical flow, discourse quality, and internal consistency.

**Implementation**: Uses the official `rouge-score` library with stemming for accurate ROUGE-1, ROUGE-2, and ROUGE-L calculations.

**Components:**

#### 1. ROUGE Metrics (24% total weight)
- **ROUGE-1 (8%)**: Unigram coherence using leave-one-out sentence comparison
- **ROUGE-2 (8%)**: Bigram coherence with optimal repetition analysis
- **ROUGE-L (8%)**: Longest common subsequence for lexical consistency

#### 2. Semantic Coherence (40% weight)
- **Sentence Embeddings**: Consecutive sentence similarity using sentence transformers
- **ROUGE-L Integration**: Combined semantic and lexical coherence analysis
- **Formula**: `0.7 * semantic_similarity + 0.3 * rouge_l_score`

#### 3. Discourse Analysis (25% weight)
- **PDTB Categories**: Penn Discourse Treebank marker analysis (temporal, contingency, comparison, expansion)
- **Marker Density**: Proportion of transitions with discourse markers
- **Marker Diversity**: Usage of different marker categories
- **Marker Consistency**: Even distribution throughout text

#### 4. Supporting Metrics (11% total weight)
- **Lexical Diversity (8%)**: Type-Token Ratio with windowed analysis for longer texts
- **Readability (3%)**: Flesch-Kincaid grade normalized: `max(0, min(1, (20-grade)/20))`

#### 5. Penalties Applied
- **Contradiction Penalty**: Enhanced detection using semantic + ROUGE analysis
- **Length Penalty**: 1.0 for normal length, minimum 0.3 for very short texts (<20 words)

**Final Formula:**
```
coherence = (0.08√órouge1 + 0.08√órouge2 + 0.08√órougeL + 0.40√ósemantic + 0.25√ódiscourse + 0.08√ólexical + 0.03√óreadability) √ó contradiction_penalty √ó length_penalty
```

**Score Ranges:**
- **0.7-1.0**: Excellent - Clear, well-structured, logically flowing
- **0.5-0.7**: Good - Generally coherent with minor flow issues
- **0.3-0.5**: Fair - Some coherence problems, may be too short/vague
- **0.0-0.3**: Poor - Incoherent, fragmented, or very low quality

## Overall Score

The overall score combines all three dimensions with configurable weights (default):

```
overall = (0.6 * accuracy) + (0.25 * completeness) + (0.15 * coherence)
```

## Technical Details

### Dependencies
- **OpenAI**: GPT-4/GPT-4o API for LLM-as-judge accuracy evaluation
- **rouge-score**: Official ROUGE implementation for coherence metrics
- **datasets**: Hugging Face datasets library for CNN/DailyMail dataset
- **sentence-transformers**: Semantic embeddings (default: 'all-MiniLM-L6-v2')
- **NLTK**: Tokenization, POS tagging, stopwords, named entity recognition
- **scikit-learn**: TF-IDF vectorization, cosine similarity
- **textstat**: Readability metrics (Flesch-Kincaid grade)
- **numpy**: Numerical operations

### Requirements
- Python >= 3.12
- OpenAI API key for accuracy evaluation

### Key Features
- **ROUGE-Enhanced Coherence**: Uses official rouge-score library with stemming for accurate lexical analysis
- **Robust NLP Pipeline**: Handles various text formats and lengths
- **Semantic Understanding**: Uses transformer-based embeddings for meaning comparison
- **Balanced Evaluation**: Considers multiple quality dimensions with sophisticated weighting
- **Configurable Weights**: Adjust importance of different metrics
- **Comprehensive Testing**: Full test suite with realistic examples

## Testing

Run the test suite:

```bash
python main.py
```

The test suite includes:
- LLM-as-judge accuracy evaluation with various summary qualities
- Completeness testing with comprehensive vs. sparse summaries
- ROUGE-enhanced coherence assessment with proper scoring thresholds
- Summary generation with different quality/length parameters
- Boundary condition testing
- Score validation (0.0-1.0 range)
- ROUGE metric validation using official library

## Dataset Evaluation

Evaluate CNN/DailyMail reference summaries:

```bash
# Evaluate 10 reference summaries with pretty output
python evalsummary.py -n 10

# Generate CSV data for analysis
python evalsummary.py -n 50 -f csv > cnn_dailymail_evaluation.csv
```

## Example Results

**Pretty Output Format:**
```
üìä SUMMARY EVALUATION RESULTS:
----------------------------------------
üéØ Accuracy:     0.667 (66.7%) - Good: Generally accurate with minor issues
üìã Completeness: 0.823 (82.3%) - Excellent: Covers most important topics
üîó Coherence:    0.512 (51.2%) - Good: ROUGE-enhanced coherence with solid discourse flow
‚≠ê Overall:      0.698 (69.8%) - High quality summary

üí≠ Accuracy Rationale: The summary accurately captures the main points...
```

**CSV Output Format:**
```csv
article_index,article_text,reference_summary,accuracy,completeness,coherence,overall,accuracy_rationale,model
42857,"Full article text...",Reference summary text,0.667,0.823,0.512,0.698,"Rationale text",gpt-4o
```

## Customization

Adjust evaluation weights for specific use cases:

```python
# Emphasize accuracy for factual content
weights = {'accuracy': 0.7, 'completeness': 0.2, 'coherence': 0.1}

results = evaluator.evaluate_summary(
    ai_summary, source_text, weights=weights
)
```

## License

MIT License