# SummaryEval

A comprehensive evaluation framework for AI-generated summaries that assesses accuracy, completeness, and coherence using NLP techniques and LLM-as-judge methodology.

## Overview

SummaryEval provides quantitative metrics to evaluate the quality of AI-generated summaries across three key dimensions:

- **Accuracy**: Uses "LLM-As-Judge" (openai models) to assess factual correctness and alignment with source material
- **Completeness**: Coverage of important topics from the source text using semantic similarity
- **Coherence**: Readability, logical flow, and internal consistency using NLP metrics

## Features

- **CNN/DailyMail Dataset Integration**: Evaluate reference summaries from the CNN/DailyMail dataset
- **Command Line Interface**: Easy-to-use CLI with flexible output formats
- **Multiple Output Formats**: Pretty console output or CSV for data analysis
- **LLM-as-Judge Accuracy**: Uses OpenAI models for sophisticated accuracy assessment
- **Configurable Parameters**: Customize sample size, model selection, and output format

## Installation

```bash
pip install -e .
```

## Environment Setup

Set your OpenAI API key:
```bash
export OPENAI_API_KEY="your-api-key-here"
```

## Quick Start

### Python API

```python
from main import SummaryEvaluator

# Initialize with your preferred OpenAI model
evaluator = SummaryEvaluator("gpt-4o")

# Evaluate a summary
results = evaluator.evaluate_summary(
    ai_summary="Your AI-generated summary here",
    source_text="Original source material here",
    debug=True  # Enable for detailed accuracy rationale
)

print(f"Overall Score: {results['overall']:.3f}")
print(f"Accuracy: {results['accuracy']:.3f}")
print(f"Completeness: {results['completeness']:.3f}")
print(f"Coherence: {results['coherence']:.3f}")
```

### Command Line Interface

Evaluate CNN/DailyMail reference summaries:

```bash
# Basic usage (5 samples, gpt-4o, pretty output)
python evalsummary.py

# Custom parameters
python evalsummary.py -n 10 -m gpt-4 -f pretty

# CSV output for data analysis
python evalsummary.py -n 20 -f csv > results.csv

# Reproducible results with seed
python evalsummary.py -n 5 --seed 42

# Show help
python evalsummary.py --help
```

**CLI Options:**
- `-n, --sample-size`: Number of random articles to evaluate (default: 5)
- `-m, --model`: OpenAI model name (default: gpt-4o)
- `-f, --format`: Output format - `pretty` or `csv` (default: pretty)
- `--seed`: Random seed for reproducible results

## Project Structure

```
summaryeval/
‚îú‚îÄ‚îÄ main.py              # Core SummaryEvaluator class and test suite
‚îú‚îÄ‚îÄ evalsummary.py       # CNN/DailyMail evaluation CLI script
‚îú‚îÄ‚îÄ pyproject.toml       # Project dependencies and metadata
‚îú‚îÄ‚îÄ README.md            # This documentation
‚îî‚îÄ‚îÄ results.csv          # Example output (generated by running CLI)
```

**Key Files:**
- **main.py**: Contains the `SummaryEvaluator` class with all evaluation methods
- **evalsummary.py**: Command-line interface for evaluating CNN/DailyMail dataset
- **pyproject.toml**: Project configuration with all required dependencies

## Scoring Methodology

### Accuracy Score (0.0 - 1.0)

Uses LLM-as-judge methodology with GPT-4/GPT-4o to assess factual correctness and alignment with source material.

**Process:**
1. **LLM Evaluation**: OpenAI model compares summary against source text
2. **Scoring Scale**: Model rates accuracy on 0-3 scale:
   - **3 (Excellent)**: Highly accurate, well-aligned with source
   - **2 (Good)**: Generally accurate with minor issues
   - **1 (Fair)**: Some accuracy problems or contradictions
   - **0 (Poor)**: Significant inaccuracies or contradictions
3. **Normalization**: Score normalized to 0-1 range (`score / 3.0`)
4. **Rationale**: Model provides reasoning for score (available in debug mode)

**Key Features:**
- Sophisticated semantic understanding via large language models
- Contextual evaluation considering nuance and intent
- Detailed rationale explaining scoring decisions
- Robust handling of complex factual relationships

### Completeness Score (0.0 - 1.0)

Measures coverage of important topics from the source material.

**Process:**
1. **Topic Extraction**: Important topics identified using TF-IDF with position and length weighting:
   - **Position weight**: First/last sentences get 1.5x weight, early/late sentences get 1.2x
   - **Length weight**: Longer sentences (up to 20 words) get higher importance
2. **Coverage Analysis**: Summary embedding compared to topic embeddings
3. **Topic Matching**: Topics covered if similarity >= 0.4 with summary
4. **Weighted Scoring**: Combines topic recall and importance weighting
5. **Final Score**: `completeness = (coverage_recall * 0.7) + (importance_weighting * 0.3)`

**Key Parameters:**
- Coverage threshold: 0.4
- Recall weight: 0.7
- Importance weight: 0.3

### Coherence Score (0.0 - 1.0)

Measures readability, logical flow, and internal consistency.

**Components:**

#### 1. Readability (30% weight)
- Uses Flesch-Kincaid grade level
- Normalized for grade levels 5-20: `(15 - max(0, grade - 5)) / 15`
- Less punitive for reasonable complexity levels

#### 2. Logical Flow (40% weight)
- **Sentence Coherence**: Consecutive sentence similarity using sentence embeddings
- **Discourse Markers**: Presence of transition words (however, therefore, furthermore, etc.)
- Combined score: `(sentence_coherence + discourse_quality) / 2`

#### 3. Consistency (30% weight)
- **Contradiction Detection**: Claims with cosine similarity < -0.2 are considered contradictory
- **Consistency Score**: `1 - (contradictions / total_claim_pairs)`

**Final Formula:**
```
coherence = (0.3 * readability) + (0.4 * logical_flow) + (0.3 * consistency)
```

## Overall Score

The overall score combines all three dimensions with configurable weights (default):

```
overall = (0.6 * accuracy) + (0.25 * completeness) + (0.15 * coherence)
```

## Technical Details

### Dependencies
- **OpenAI**: GPT-4/GPT-4o API for LLM-as-judge accuracy evaluation
- **datasets**: Hugging Face datasets library for CNN/DailyMail dataset
- **sentence-transformers**: Semantic embeddings (default: 'all-MiniLM-L6-v2')
- **NLTK**: Tokenization, POS tagging, stopwords, named entity recognition
- **scikit-learn**: TF-IDF vectorization, cosine similarity
- **textstat**: Readability metrics (Flesch-Kincaid grade)
- **numpy**: Numerical operations

### Requirements
- Python >= 3.12
- OpenAI API key for accuracy evaluation

### Key Features
- **Robust NLP Pipeline**: Handles various text formats and lengths
- **Semantic Understanding**: Uses transformer-based embeddings for meaning comparison
- **Balanced Evaluation**: Considers multiple quality dimensions
- **Configurable Weights**: Adjust importance of different metrics
- **Comprehensive Testing**: Full test suite with realistic examples

## Testing

Run the test suite:

```bash
python main.py
```

The test suite includes:
- LLM-as-judge accuracy evaluation with various summary qualities
- Completeness testing with comprehensive vs. sparse summaries
- Coherence assessment across readability levels
- Summary generation with different quality/length parameters
- Boundary condition testing
- Score validation (0.0-1.0 range)

## Dataset Evaluation

Evaluate CNN/DailyMail reference summaries:

```bash
# Evaluate 10 reference summaries with pretty output
python evalsummary.py -n 10

# Generate CSV data for analysis
python evalsummary.py -n 50 -f csv > cnn_dailymail_evaluation.csv
```

## Example Results

**Pretty Output Format:**
```
üìä SUMMARY EVALUATION RESULTS:
----------------------------------------
üéØ Accuracy:     0.667 (66.7%) - Good: Generally accurate with minor issues
üìã Completeness: 0.823 (82.3%) - Excellent: Covers most important topics
üîó Coherence:    0.741 (74.1%) - Good: Clear structure and flow
‚≠ê Overall:      0.721 (72.1%) - High quality summary

üí≠ Accuracy Rationale: The summary accurately captures the main points...
```

**CSV Output Format:**
```csv
article_index,article_text,reference_summary,accuracy,completeness,coherence,overall,accuracy_rationale,model
42857,"Full article text...",Reference summary text,0.667,0.823,0.741,0.721,"Rationale text",gpt-4o
```

## Customization

Adjust evaluation weights for specific use cases:

```python
# Emphasize accuracy for factual content
weights = {'accuracy': 0.7, 'completeness': 0.2, 'coherence': 0.1}

results = evaluator.evaluate_summary(
    ai_summary, source_text, weights=weights
)
```

## License

MIT License